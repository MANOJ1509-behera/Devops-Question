```

1. How would you diagnose and resolve a system that is running low on disk space?

Commands: df -h, du -sh *, Isof | grep deleted

Explanation: Use df to check disk usage, du to check directory sizes, and Isof to find open files that are consuming space.

2. How do you troubleshoot a system that is running out of memory?

Commands: free -m, top, vmstat, dmesg | grep -i oom

Explanation: Check memory usage with free and top.

Use vmstat to analyze memory statistics and look for OOM (Out of Memory) errors in dmesg.

3. The system is slow and users are complaining about performance. What steps would you take to troubleshoot?

Commands: top, htop, iotop, vmstat, sar

Explanation: Check CPU, memory, disk, and I/O usage with top and iotop. Use vmstat and sar for historical data analysis.

4. The system is experiencing disk I/O issues. How would you troubleshoot?

Commands: iostat, iotop, dstat, vmstat

Explanation: Use iostat to monitor disk I/O statistics.

iotop shows I/O usage by processes. dstat provides detailed performance metrics for various resources.

5. How do you check for a full filesystem and resolve it?

Commands: df -h, du -sh *, Isof | grep deleted,

journalctl

Explanation: Use df to check filesystem usage, du to analyze space consumption by directories, and Isof to detect open but deleted files.

6. The system is not resolving domain names. How would you troubleshoot DNS issues?

Commands: nslookup, dig, cat /etc/resolv.conf, systemctl restart network

Explanation: Use nslookup and dig to check DNS resolution. Verify /etc/resolv.conf for correct nameservers. Restart networking if necessary.

7. A critical Linux process died out of no where!
Has this ever happened to you?

As a DevOps engineer, I've dealt with this exact scenario in test/production environments. 

When it happens, you need to diagnose and resolve the issue ASAP. 

Here's how I investigate these issues:

✅Step 1: Check if the process is still running
-> ps aux | grep process_name to see if it crashed
-> pgrep -fl process_name to double-check if it's in memory
-> dmesg -T | tail -50 to look for segfaults or OOM kills

✅Step 2: Scan system logs for clues
-> journalctl -xe --no-pager -n 50 for errors before the process stopped
-> tail -f /var/log/syslog for warnings or crash messages

✅Step 3: Analyze CPU & memory usage
-> top -o %CPU to check for high CPU consumption
-> top -o %MEM to see if it hit memory limits
-> dmesg | grep -i "oom" to determine if the OOM Killer terminated it

✅ Step 4: Investigate disk issues
-> df -h to check if the disk is full and preventing writes
-> iostat -xm 1 to look for disk I/O bottlenecks that could've frozen the process
-> dmesg | grep -i "error" to find any file system errors

✅Step 5: Look for locked or deleted files
-> lsof -p <PID> to see if the process is stuck on a locked file
-> lsof | grep -iE "deleted|locked" to detect lingering file issues

✅Step 6: Determine if an external source killed the process
-> journalctl -u process_name --no-pager -n 50 to check for manual kills
-> lastcomm | grep process_name to find out who/what sent the kill signal

✅Step 7: Debug in real-time
-> strace -p <PID> to see what syscalls the process is making
-> gdb -p <PID> to attach and inspect the process state


```
